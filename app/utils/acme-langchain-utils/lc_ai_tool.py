import operator
from typing import List, Literal, TypedDict, Annotated, Optional
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import BaseMessage
from langchain_core.tools import BaseTool

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver


# ============================================================
# 1. é…ç½®å±‚
# ============================================================

class ResearchAgentConfig(BaseModel):
    """é…ç½®ç±»ï¼šå®šä¹‰ Agent çš„è¡Œä¸ºè¾¹ç•Œå’Œä¾èµ–"""

    # æ ¸å¿ƒä¾èµ–
    llm: BaseChatModel = Field(description="ä½¿ç”¨çš„ LLM æ¨¡å‹å®ä¾‹")
    search_tool: BaseTool = Field(description="ä½¿ç”¨çš„æœç´¢å·¥å…·å®ä¾‹")

    # è¿è¡Œå‚æ•°
    max_iterations: int = Field(default=3, description="æœ€å¤§è‡ªæˆ‘è¿­ä»£æ¬¡æ•°")
    temperature: float = Field(default=0.0, description="LLM ç”Ÿæˆæ¸©åº¦")

    # æç¤ºè¯æ¨¡æ¿ (å…è®¸å¤–éƒ¨è‡ªå®šä¹‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤)
    writer_system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©ç†ã€‚"
    critic_system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ç¼–è¾‘ã€‚"


# ============================================================
# 2. å†…éƒ¨çŠ¶æ€å±‚
# ============================================================

class _InternalState(TypedDict):
    """å†…éƒ¨çŠ¶æ€å›¾ç»“æ„ï¼Œä¸å¯¹å¤–æš´éœ²ç»†èŠ‚"""
    topic: str
    search_query: str
    research_data: str
    draft_content: str
    critique_feedback: str
    iteration_count: int
    messages: Annotated[List[BaseMessage], operator.add]


# ============================================================
# 3. æ ¸å¿ƒé€»è¾‘ç±»
# ============================================================

class AutoResearchAgent:
    """
    è‡ªä¸»ç ”ç©¶åŠ©ç† Agentã€‚

    å°è£…äº† LangGraph çš„æ„å»ºé€»è¾‘ï¼Œå¯¹å¤–æä¾›ç®€å•çš„ run æ¥å£ã€‚
    """

    def __init__(self, config: ResearchAgentConfig):
        self.config = config
        self.graph = self._build_graph()
        # å¯é€‰ï¼šæ·»åŠ  Checkpoint ä»¥æ”¯æŒå†…å­˜/æŒä¹…åŒ–
        self.memory = MemorySaver()

    def _get_nodes(self):
        """å®šä¹‰å›¾çš„æ‰€æœ‰èŠ‚ç‚¹"""

        def researcher(state: _InternalState):
            print(f"ğŸ” [Node: Researcher] æœç´¢ä¸­: {state['search_query']}")
            res = self.config.search_tool.invoke(state["search_query"])
            # ç®€å•çš„æ•°æ®æ¸…æ´—
            formatted = "\n".join([f"{r.get('title', '')}: {r.get('content', '')}" for r in res])
            return {"research_data": formatted}

        def writer(state: _InternalState):
            print(f"âœï¸  [Node: Writer] æ’°å†™ä¸­ (ç¬¬ {state['iteration_count']} ç‰ˆ)...")

            prompt = ChatPromptTemplate.from_messages([
                ("system", self.config.writer_system_prompt),
                ("human", "ä¸»é¢˜: {topic}\nç°æœ‰è‰ç¨¿:\n{existing_draft}\næ–°èµ„æ–™:\n{research_data}\n\nè¯·æ•´åˆå¹¶æ›´æ–°è‰ç¨¿ã€‚")
            ])

            chain = prompt | self.config.llm | StrOutputParser()
            response = chain.invoke({
                "topic": state["topic"],
                "existing_draft": state.get("draft_content", "æš‚æ— è‰ç¨¿"),
                "research_data": state["research_data"]
            })
            return {"draft_content": response}

        def critic(state: _InternalState):
            print(f"ğŸ” [Node: Critic] å®¡æŸ¥ä¸­...")

            prompt = ChatPromptTemplate.from_messages([
                ("system", self.config.critic_system_prompt),
                ("human", "è‰ç¨¿:\n{draft_content}\n\nå¦‚æœå®Œç¾ï¼Œå›å¤ 'PASS'ã€‚å¦‚æœä¸å®Œç¾ï¼Œå›å¤ 'CONTINUE: <å»ºè®®çš„æœç´¢è¯>'ã€‚")
            ])

            chain = prompt | self.config.llm | StrOutputParser()
            feedback = chain.invoke({"draft_content": state["draft_content"]})

            if "PASS" in feedback:
                return {"critique_feedback": "PASS"}

            # æå–æ–°çš„æœç´¢æŒ‡ä»¤
            new_query = feedback.replace("CONTINUE:", "").strip()
            if not new_query: new_query = state["topic"] + " æ›´å¤šç»†èŠ‚"

            print(f"âš ï¸  å®¡æŸ¥æ„è§: éœ€è¦è¡¥å…… '{new_query}'")
            return {
                "critique_feedback": "CONTINUE",
                "search_query": new_query
            }

        return researcher, writer, critic

    def _should_continue(self, state: _InternalState) -> Literal["researcher", END]:
        """è¾¹çš„æ¡ä»¶é€»è¾‘ï¼šå†³å®šæ˜¯å¾ªç¯è¿˜æ˜¯ç»“æŸ"""
        if state.get("critique_feedback") == "PASS":
            print("âœ… å®¡æŸ¥é€šè¿‡ï¼Œæµç¨‹ç»“æŸã€‚")
            return END

        if state["iteration_count"] >= self.config.max_iterations:
            print(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({self.config.max_iterations})ï¼Œå¼ºåˆ¶ç»“æŸã€‚")
            return END

        return "researcher"

    def _build_graph(self) -> StateGraph:
        """æ„å»º LangGraph æœ‰å‘å›¾"""
        workflow = StateGraph(_InternalState)

        researcher, writer, critic = self._get_nodes()

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("researcher", researcher)
        workflow.add_node("writer", writer)
        workflow.add_node("critic", critic)

        # è®¾ç½®å…¥å£
        workflow.set_entry_point("researcher")

        # å®šä¹‰è¾¹
        workflow.add_edge("researcher", "writer")
        workflow.add_edge("writer", "critic")

        # æ¡ä»¶è¾¹ï¼ˆå¾ªç¯ï¼‰
        workflow.add_conditional_edges(
            "critic",
            self._should_continue,
            {
                "researcher": "researcher",
                END: END
            }
        )

        # ç¼–è¯‘ (å¸¦ Checkpoint æ”¯æŒçš„è¯å¯ä»¥ç”¨ workflow.compile(checkpointer=self.memory))
        return workflow.compile(checkpointer=self.memory)

    def run(
            self,
            topic: str,
            initial_query: Optional[str] = None,
            thread_id: str = "default_session"
    ) -> str:
        """
        å¯¹å¤–æš´éœ²çš„è¿è¡Œæ¥å£ã€‚

        Args:
            topic: ç ”ç©¶ä¸»é¢˜
            initial_query: åˆå§‹æœç´¢è¯ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º topicï¼‰
            thread_id: ä¼šè¯ IDï¼Œç”¨äºè®°å¿†ä¿å­˜

        Returns:
            str: æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Š
        """
        if not initial_query:
            initial_query = topic

        config = RunnableConfig(configurable={"thread_id": thread_id})

        initial_state: _InternalState = {
            "topic": topic,
            "search_query": initial_query,
            "research_data": "",
            "draft_content": "",
            "critique_feedback": "",
            "iteration_count": 1,
            "messages": []
        }

        print(f"ğŸš€ å¼€å§‹ä»»åŠ¡: {topic}")

        # æ‰§è¡Œå›¾
        final_state = self.graph.invoke(initial_state, config)

        return final_state.get("draft_content", "ç”Ÿæˆå¤±è´¥")


# ============================================================
# 4. ä½¿ç”¨ç¤ºä¾‹
# ============================================================

if __name__ == "__main__":
    from langchain_community.tools.tavily_search import TavilySearchResults
    import os

    # 1. å‡†å¤‡é…ç½®
    # æ³¨æ„ï¼šè¿™é‡Œå¯ä»¥è½»æ¾æ›¿æ¢ä¸ºå…¶ä»– LLM (å¦‚ Anthropic, Ollama) æˆ–å…¶ä»– Tool
    config = ResearchAgentConfig(
        llm=ChatOpenAI(model="gpt-4o", temperature=0),
        search_tool=TavilySearchResults(max_results=3),
        max_iterations=3,
        writer_system_prompt="ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€åšä¸»ï¼Œé£æ ¼è¦å¹½é»˜é£è¶£ã€‚"
    )

    # 2. å®ä¾‹åŒ– Agent
    agent = AutoResearchAgent(config)

    # 3. æ‰§è¡Œ
    report = agent.run(topic="é‡å­è®¡ç®—åœ¨ 2024 å¹´çš„çªç ´")

    print("\n" + "=" * 30 + " æœ€ç»ˆæŠ¥å‘Š " + "=" * 30)
    print(report)